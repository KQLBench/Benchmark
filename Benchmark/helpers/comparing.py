from typing import List, Set
from Benchmark.models.benchmark_models import BenchmarkResult, TestCase
import textwrap

# Added imports
import os
import sys
import json # Standard utility, though not directly used in the final prompt formatting
import litellm
from pydantic import BaseModel
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient
import dotenv
from Benchmark.configuration.models_config import MODELS_CONFIG
from Benchmark.helpers.litellm_client_setup import setup_litellm_client

# Load environment variables from .env file if present
dotenv.load_dotenv()

# Setup LiteLLM client for semantic comparison using first model in MODELS_CONFIG
_COMPARE_MODEL_KEY = list(MODELS_CONFIG.keys())[0]
try:
    _VAULT_URL = os.getenv("VAULT_URL")
    _CREDENTIAL_COMPARE = DefaultAzureCredential()
    _SECRET_CLIENT_COMPARE = SecretClient(vault_url=_VAULT_URL, credential=_CREDENTIAL_COMPARE)
    setup_litellm_client(_SECRET_CLIENT_COMPARE, _COMPARE_MODEL_KEY)
    _COMPARE_MODEL_ID = MODELS_CONFIG[_COMPARE_MODEL_KEY]["model"]
except Exception as e:
    print(f"LiteLLM semantic comparison setup failed: {e}", file=sys.stderr)
    _COMPARE_MODEL_ID = None

class AnswerVerification(BaseModel):
    is_equivalent: bool
    reasoning: str

def convert_to_set(results: List[List]) -> Set[tuple]:
    """Convert query results to a set of tuples for comparison"""
    if not results or len(results) <= 1:  # No results or only headers
        return set()
    
    # Convert all rows (except headers) to tuples for set operations
    return {tuple(row) for row in results[1:]}

def compare_KQLs(results1: List[List], results2: List[List]) -> float:
    """
    Compare two KQL query results using Jaccard similarity
    
    Args:
        results1: First query results as list of lists (including headers)
        results2: Second query results as list of lists (including headers)
        
    Returns:
        Float between 0 and 1 indicating similarity (1 = identical, 0 = completely different)
    """
    if not results1 or not results2:
        return 0.0
        
    # Convert results to sets for comparison
    set1 = convert_to_set(results1)
    set2 = convert_to_set(results2)
    
    # Handle empty result sets
    if not set1 or not set2:
        return 0.0
    
    # Calculate Jaccard similarity: intersection / union
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    
    return intersection / union if union > 0 else 0.0

def compare_answer(answer_model: str, answer_ground_truth: any, test_case: TestCase) -> int:
    """
    Compare model answer to ground truth answer.
    The comparison is case-insensitive. First, it checks if the ground truth answer
    is contained within the model's answer. If not, and the model provided an answer,
    it uses an LLM for semantic comparison.

    Args:
        answer_model: Answer generated by the model (string)
        answer_ground_truth: Ground truth answer (string or list of strings)
        test_case: The TestCase object with details about the question.

    Returns:
        1 if the ground_truth is found in the model_answer (case-insensitive)
        or if LLM confirms semantic equivalence, 0 otherwise.
    """
    # Robust handling of model answer type
    if not isinstance(answer_model, str):
        # If answer_model is None, or not a string, it can't "contain an answer" for comparison.
        # This implies a mismatch unless ground_truth is also None/empty in a specific way.
        # The original code would have crashed if answer_model was None.
        return 0

    model_answer_lower = answer_model.lower()
    model_answer_lower_stripped = model_answer_lower.strip()
    initial_match = 0

    # Perform initial direct comparison (more robustly than original)
    if isinstance(answer_ground_truth, list):
        # Filter for actual, non-empty strings in the ground truth list
        str_truths = [str(truth).lower() for truth in answer_ground_truth if isinstance(truth, str) and str(truth).strip()]
        if str_truths: # If there are valid, non-empty string ground truths
            initial_match = 1 if any(truth in model_answer_lower for truth in str_truths) else 0
        elif not model_answer_lower_stripped: # GT list effectively empty, and model answer also empty/whitespace
             initial_match = 1
        # else: GT list effectively empty, model answer not empty -> initial_match remains 0
    elif isinstance(answer_ground_truth, str):
        gt_lower = answer_ground_truth.lower()
        gt_lower_stripped = gt_lower.strip()

        if not gt_lower_stripped and not model_answer_lower_stripped: # Both effectively empty
             initial_match = 1
        elif gt_lower_stripped: # Ground truth is a non-empty string
            # Original logic: check if ground truth is *contained within* model's answer
            initial_match = 1 if gt_lower in model_answer_lower else 0
        # else: gt_lower_stripped is empty, but model_answer_lower_stripped is not -> initial_match remains 0
    else:
        # Ground truth is not a string or list (e.g., None, int).
        # Match if ground_truth is None and model answer is also empty/whitespace.
        if answer_ground_truth is None and not model_answer_lower_stripped:
            initial_match = 1
        # else: Non-comparable GT type and model answer exists, or GT is None and model answer exists -> initial_match remains 0

    if initial_match == 1:
        return 1

    if answer_model.strip():
        # Semantic comparison using LiteLLM and JSON structured output
        if not _COMPARE_MODEL_ID:
            return 0
        # Prepare ground truth string
        answer_ground_truth_str = ""
        if isinstance(answer_ground_truth, list):
            valid_gt_parts = [str(gt) for gt in answer_ground_truth if gt is not None and isinstance(gt, str) and str(gt).strip()]
            answer_ground_truth_str = ", ".join(valid_gt_parts)
        elif isinstance(answer_ground_truth, str):
            answer_ground_truth_str = answer_ground_truth
        elif answer_ground_truth is not None:
            answer_ground_truth_str = str(answer_ground_truth)
        if not answer_ground_truth_str.strip():
            return 0

        # Tool schema for AnswerVerification
        answer_verification_tool_schema = {
            "type": "function",
            "function": {
                "name": "record_answer_equivalency",
                "description": "Records whether the model's answer is semantically equivalent to the ground truth, along with reasoning.",
                "parameters": AnswerVerification.model_json_schema()
            }
        }

        test_case_details_str = f"""
Question (Prompt): {test_case.prompt}
Technique ID: {test_case.technique_id}
Objective: {test_case.objective}
Context: {test_case.context}
"""

        system_prompt_compare = (
            "You are an expert assistant. Your task is to determine if the Model's Answer is semantically "
            "equivalent to or correctly addresses the question based on the Ground Truth Answer(s) and the original Test Case Details. "
            "Use the 'record_answer_equivalency' tool to provide your assessment."
        )
        user_prompt_compare = f"""
Test Case Details:
{test_case_details_str}

Model's Answer:
'''{answer_model}'''

Ground Truth Answer(s):
'''{answer_ground_truth_str}'''

Does the Model's Answer correctly match the Ground Truth given the Test Case Details?"""

        messages = [
            {"role": "system", "content": system_prompt_compare},
            {"role": "user", "content": user_prompt_compare}
        ]
        try:
            raw_response = litellm.completion(
                model=_COMPARE_MODEL_ID,
                messages=messages,
                tools=[answer_verification_tool_schema],
                tool_choice={"type": "function", "function": {"name": "record_answer_equivalency"}},
                temperature=0
            )
            # Extract arguments from tool call
            if raw_response.choices and raw_response.choices[0].message.tool_calls:
                tool_call = raw_response.choices[0].message.tool_calls[0]
                if tool_call.function.name == "record_answer_equivalency":
                    arguments_str = tool_call.function.arguments
                    try:
                        parsed_args = json.loads(arguments_str)
                        verification_result = AnswerVerification(**parsed_args)
                        return 1 if verification_result.is_equivalent else 0
                    except json.JSONDecodeError as e:
                        print(f"Failed to parse JSON arguments from LLM for tool 'record_answer_equivalency': {arguments_str}. Error: {e}", file=sys.stderr)
                        return 0
                else:
                    print(f"LLM called unexpected tool: {tool_call.function.name}. Expected 'record_answer_equivalency'.", file=sys.stderr)
                    return 0
            else:
                print("LLM did not make the expected tool call to 'record_answer_equivalency'.", file=sys.stderr)
                # Fallback: attempt to parse content if available, as a last resort
                if raw_response.choices and raw_response.choices[0].message.content:
                    content = raw_response.choices[0].message.content
                    try:
                        parsed_content = json.loads(content)
                        if isinstance(parsed_content, dict) and 'is_equivalent' in parsed_content:
                             verification_result = AnswerVerification(**parsed_content)
                             print("Warning: Parsed content as fallback as tool call was missing.", file=sys.stderr)
                             return 1 if verification_result.is_equivalent else 0
                    except json.JSONDecodeError:
                        print(f"Content was not valid JSON either: {content}", file=sys.stderr)
                return 0
        except Exception as e:
            print(f"Error during semantic comparison with LiteLLM: {e}", file=sys.stderr)
            return 0
    
    # Fallback if:
    # 1. Initial match failed (initial_match was 0)
    # 2. AND EITHER:
    #    a. Model answer was empty/whitespace (so LLM part was skipped)
    #    b. LLM part was attempted but failed (client issue, API error, parsing error)
    return 0

def print_comparison_table(benchmark_result: BenchmarkResult) -> None:
    """
    Print a formatted comparison table of LLM vs expected results
    
    Args:
        benchmark_result: BenchmarkResult object containing test results
    """
    if not benchmark_result.test_results:
        print("No test results to display")
        return
    
    # Format for the table
    header = "| {:<15} | {:<15} | {:<40} | {:<40} | {:<8} | {:<8} |".format(
        "Technique", "Test ID", "Expected Answer", "LLM Answer", "Correct", "Attempts"
    )
    separator = "-" * len(header)
    
    print(separator)
    print(header)
    print(separator)
    
    # Sort results by technique ID
    sorted_results = sorted(benchmark_result.test_results, key=lambda r: r.test_case.technique_id)
    
    for result in sorted_results:
        # Get expected answer(s)
        expected = result.test_case.answer # Corrected: result.test_case.answer instead of result.question.answer
        if isinstance(expected, list):
            expected_str = ", ".join(expected)
        else:
            expected_str = str(expected)
            
        # Format columns with proper width
        technique = result.test_case.technique_id
        test_id = result.test_case.question_id[:8]  # Corrected: result.test_case.question_id instead of result.test_case.guid
        expected_formatted = textwrap.shorten(expected_str, width=37, placeholder="...")
        llm_answer = textwrap.shorten(result.query_result.answer, width=37, placeholder="...") 
        correct = "✓" if result.answer_correct else "✗"
        attempts = result.query_result.attempts
        
        row = "| {:<15} | {:<15} | {:<40} | {:<40} | {:<8} | {:<8} |".format(
            technique, test_id, expected_formatted, llm_answer, correct, attempts
        )
        print(row)
    
    print(separator)
    
    # Print statistical summary for this model
    correct = sum(1 for r in benchmark_result.test_results if r.answer_correct)
    total = len(benchmark_result.test_results)
    
    # Calculate average attempts
    attempts = [r.query_result.attempts for r in benchmark_result.test_results 
               if r.query_result and r.query_result.attempts]
    avg_attempts = sum(attempts) / len(attempts) if attempts else 0
    max_attempts = benchmark_result.configuration.get('tries_used', 0) # Use .get() for safety
    
    print(f"Total correct: {correct}/{total} ({correct/total*100:.1f}%)")
    # Ensure max_attempts is not zero to avoid DivisionByZeroError if tries_used was somehow not set or 0
    if max_attempts > 0:
        print(f"Average attempts: {avg_attempts:.2f}/{max_attempts} ({avg_attempts/max_attempts*100:.1f}%)")
    else:
        print(f"Average attempts: {avg_attempts:.2f}/{max_attempts} (N/A %)")
    print(separator)